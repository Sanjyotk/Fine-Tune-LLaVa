{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+eeMVMw8WZPwfUNs+ca3j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Fine-tune and deploy the multimodal LLaVA model with DeepSpeed🤙\n","Hi everyone!\n","\n","In this notebook we'll fine-tune the LLaVA model. LLaVA is multimodal which means it can ingest and understand images along with text! LLaVA comes from a research paper titled Visual Instruction Tuning and introduces the Large Language and Vision Assistant methodology. In order to process images, LLaVA relies on the pre-trained CLIP visual encoder ViT-L/14 which maps images and text into the same latent space.\n","\n","Help us make this tutorial better! Please provide feedback on the Discord channel or on X.\n","\n","Table of contents\n","Data Preprocessing\n","LLaVA Installation\n","DeepSpeed configuration\n","Weights and Biases\n","Finetuning flow\n","Deployment via gradio interface\n","Data Preprocessing\n","LLaVA requires data to be in a very specific format. Below we use a helper function to format the OKV-QA dataset. This dataset teaches the model to respond to an image in short phrases without any preamble or extra verbiage.\n","\n","Optional: create your own dataset\n","The guide to creating your own dataset is relatively simple! Here's a simple script that you could use that leverages GPT4's multimodal capabilities to quickly create a dataset that can be used in the dataset creator function that we write below!"],"metadata":{"id":"VzL7vOPUupNH"}},{"cell_type":"code","source":["import openai\n","import os\n","from PIL import Image\n","import json\n","\n","# Initialize OpenAI client\n","openai.api_key = 'your-api-key'\n","\n","def generate_description(text_description):\n","    try:\n","        response = openai.Completion.create(\n","            engine=\"gpt-4\",\n","            prompt=text_description,\n","            max_tokens=150\n","        )\n","        return response.choices[0].text.strip()\n","    except Exception as e:\n","        print(f\"Error in generating description: {e}\")\n","        return None\n","\n","def process_images(image_folder, output_folder):\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","    json_data_list = []\n","\n","    for filename in os.listdir(image_folder):\n","        if filename.endswith((\".png\", \".jpg\", \".jpeg\")):\n","            image_path = os.path.join(image_folder, filename)\n","            try:\n","                # Open and possibly display the image here if needed\n","                image = Image.open(image_path)\n","                image.show()  # You can comment this line if you do not want to display the image\n","\n","                # Generate the textual description from GPT-4\n","                description_output = generate_description(description_input)\n","\n","                # Save the description and the image path in a JSON structure\n","                json_data = {\n","                    \"image\": filename,\n","                    \"description\": description_output\n","                }\n","                json_data_list.append(json_data)\n","\n","            except Exception as e:\n","                print(f\"Error processing image {filename}: {e}\")\n","\n","    # Save all data to a JSON file\n","    json_output_path = os.path.join(output_folder, 'descriptions.json')\n","    with open(json_output_path, 'w') as json_file:\n","        json.dump(json_data_list, json_file, indent=4)\n","\n","# Usage\n","image_folder = 'path_to_your_image_folder'\n","output_folder = 'path_to_your_output_folder'\n","process_images(image_folder, output_folder)\n"],"metadata":{"id":"HwFB3ep7uq2P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How to use this script\n","\n","Place all your images in a directory and specify that path as image_folder.\n","Provide descriptions manually when prompted; these will be used by GPT-4 to generate detailed text outputs.\n","Outputs are saved in a JSON file in the specified output_folder, pairing each image file with its generated description.\n","Back to it!\n"],"metadata":{"id":"yxRS_sY0uv90"}},{"cell_type":"code","source":["# Install preprocessing libraries\n","!pip install datasets\n","!pip install --upgrade --force-reinstall Pillow"],"metadata":{"id":"DJ-VxgRuuwvL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imports\n","from datasets import load_dataset\n","from PIL import Image\n","from io import BytesIO\n","import requests\n","import os\n","import json\n","import uuid\n","\n","# Check PIL import\n","import PIL.Image\n","\n","# Define preprocessing functions\n","def process_and_save(dataset, output_folder, subset_name):\n","    # Define image subfolder within output folder\n","    subset_folder = os.path.join(output_folder, subset_name)\n","    image_subfolder = os.path.join(output_folder, 'images')\n","\n","    if not os.path.exists(image_subfolder):\n","        os.makedirs(image_subfolder)\n","\n","    if not os.path.exists(subset_folder):\n","        os.makedirs(subset_folder)\n","\n","    # Initialize list to hold all JSON data\n","    json_data_list = []\n","\n","    # Process and save images and labels\n","    for item in dataset:\n","        # Load image if it's a URL or a file path\n","        if isinstance(item['image'], str):\n","            response = requests.get(item['image'])\n","            image = Image.open(BytesIO(response.content))\n","        else:\n","            image = item['image']  # Assuming it's a PIL.Image object\n","\n","        # Create a unique ID for each image\n","        unique_id = str(uuid.uuid4())\n","\n","        # Define image path\n","        image_path = os.path.join(image_subfolder, f\"{unique_id}.jpg\")\n","\n","        # Save image\n","        image.save(image_path)\n","\n","        # Remove duplicates and format answers\n","        answers = item['answers']\n","        unique_answers = list(set(answers))\n","        formatted_answers = \", \".join(unique_answers)\n","\n","        # Structure for LLaVA JSON\n","        json_data = {\n","            \"id\": unique_id,\n","            \"image\": f\"{unique_id}.jpg\",\n","            \"conversations\": [\n","                {\n","                    \"from\": \"human\",\n","                    \"value\": item['question']\n","                },\n","                {\n","                    \"from\": \"gpt\",\n","                    \"value\": formatted_answers\n","                }\n","            ]\n","        }\n","\n","        # Append to list\n","        json_data_list.append(json_data)\n","\n","    # Save the JSON data list to a file\n","    json_output_path = os.path.join(output_folder, subset_name, 'dataset.json')\n","    with open(json_output_path, 'w') as json_file:\n","        json.dump(json_data_list, json_file, indent=4)\n","\n","def save_dataset(dataset_name, output_folder, class_name, subset_name, val_samples=None):\n","    # Load the dataset from Hugging Face\n","    dataset = load_dataset(dataset_name, split=subset_name)\n","\n","    # Filter for images with the specified class in 'question_type'\n","    filtered_dataset = [item for item in dataset if item['question_type'] == class_name]\n","\n","    # Determine the split for training and validation\n","    if val_samples is not None and subset_name == 'train':\n","        train_dataset = filtered_dataset[val_samples:]\n","        val_dataset = filtered_dataset[:val_samples]\n","    else:\n","        train_dataset = filtered_dataset\n","        val_dataset = []\n","\n","    # Process and save the datasets\n","    for subset, data in [('train', train_dataset), ('validation', val_dataset)]:\n","        if data:\n","            process_and_save(data, output_folder, subset)\n"],"metadata":{"id":"ZKFpACLBu0Jo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create dataset\n","output_folder = 'dataset'\n","class_name = 'other'\n","val_samples = 300\n","save_dataset('Multimodal-Fatima/OK-VQA_train', output_folder, class_name, 'train', val_samples)\n","save_dataset('Multimodal-Fatima/OK-VQA_test', output_folder, class_name, 'test')"],"metadata":{"id":"SMpMyLZBu31t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Install LLaVA\n","To install the functions needed to use the model, we have to clone the original LLaVA repository and and install it in editable mode. This lets us access all functions and helper methods"],"metadata":{"id":"ItOS5iEzu8We"}},{"cell_type":"code","source":["# The pip install -e . lets us install the repository in editable mode\n","!git clone https://github.com/haotian-liu/LLaVA.git\n","!cd LLaVA && pip install --upgrade pip && pip install -e ."],"metadata":{"id":"KMhkDd7hu9Ji"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DeepSpeed\n","Microsoft DeepSpeed is a deep learning optimization library designed to enhance the training speed and scalability of large-scale artificial intelligence (AI) models. Developed by Microsoft, this open-source tool specifically addresses the challenges associated with training very large models, allowing for reduced computational times and resource usage. By optimizing memory management and introducing novel parallelism techniques, DeepSpeed enables developers and researchers to train models with billions of parameters efficiently, even on limited hardware setups.DeepSpeed API is a lightweight wrapper on PyTorch. DeepSpeed manages all of the boilerplate training techniques, such as distributed training, mixed precision, gradient accumulation, and checkpoints and allows you to just focus on model development. To learn more about DeepSpeed and how it performs the magic, check out this article on DeepSpeed and ZeRO.\n","\n","Using deepspeed is extremely simple - you simply pip install it! The LLaVA respository contains the setup scripts and configuration files needed to finetune in different ways."],"metadata":{"id":"woSbqHpNvDQQ"}},{"cell_type":"code","source":["!cd LLaVA && pip install -e \".[train]\"\n","!pip install flash-attn --no-build-isolation"],"metadata":{"id":"veq1QT2ZvG4h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install deepspeed"],"metadata":{"id":"_Yezt-LFvIuU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Weights and Biases\n","Weights and Biases is an industry standard MLOps tool to used to monitor and evaluate training jobs. At Brev, we use Weights and Biases to track all of our finetuning jobs! Its extremely easy to setup and plugs into the DeepSpeed training loop. You simply create an account and use the cells below to log in!"],"metadata":{"id":"TnoP10RNvLJm"}},{"cell_type":"code","source":["!pip install wandb\n","\n","import wandb\n","\n","wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KisLC4gWvNSB","executionInfo":{"status":"ok","timestamp":1716103492257,"user_tz":-330,"elapsed":5038,"user":{"displayName":"SANJYOT KOTGIRE","userId":"17336819427167004907"}},"outputId":"aba7a318-8dc9-4937-c304-eb81e08897b0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wandb\n","  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.1)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-2.2.0-py2.py3-none-any.whl (281 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.1/281.1 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"markdown","source":["Finetuning job\n","Below we start the DeepSpeed training run for 5 epochs. It will automatically recognize multiple GPUs and parallelize across them. Most of the input flags are standard but you can adjust your training run with the num_train_epochs and per_device_train_batch_size flags!"],"metadata":{"id":"Iy8y5VP0vXHH"}},{"cell_type":"code","source":["!deepspeed LLaVA/llava/train/train_mem.py \\\n","    --lora_enable True --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 \\\n","    --deepspeed LLaVA/scripts/zero3.json \\\n","    --model_name_or_path liuhaotian/llava-v1.5-13b \\\n","    --version v1 \\\n","    --data_path ./dataset/train/dataset.json \\\n","    --image_folder ./dataset/images \\\n","    --vision_tower openai/clip-vit-large-patch14-336 \\\n","    --mm_projector_type mlp2x_gelu \\\n","    --mm_vision_select_layer -2 \\\n","    --mm_use_im_start_end False \\\n","    --mm_use_im_patch_token False \\\n","    --image_aspect_ratio pad \\\n","    --group_by_modality_length True \\\n","    --bf16 True \\\n","    --output_dir ./checkpoints/llava-v1.5-13b-task-lora \\\n","    --num_train_epochs 1 \\\n","    --per_device_train_batch_size 16 \\\n","    --per_device_eval_batch_size 4 \\\n","    --gradient_accumulation_steps 1 \\\n","    --evaluation_strategy \"no\" \\\n","    --save_strategy \"steps\" \\\n","    --save_steps 50000 \\\n","    --save_total_limit 1 \\\n","    --learning_rate 2e-4 \\\n","    --weight_decay 0. \\\n","    --warmup_ratio 0.03 \\\n","    --lr_scheduler_type \"cosine\" \\\n","    --logging_steps 1 \\\n","    --tf32 True \\\n","    --model_max_length 2048 \\\n","    --gradient_checkpointing True \\\n","    --dataloader_num_workers 4 \\\n","    --lazy_preprocess True \\\n","    --report_to wandb\n"],"metadata":{"id":"O3yZEEeKvPJ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# merge the LoRA weights with the full model\n","!python LLaVA/scripts/merge_lora_weights.py --model-path checkpoints/llava-v1.5-13b-task-lora --model-base liuhaotian/llava-v1.5-13b --save-model-path llava-ftmodel"],"metadata":{"id":"ufHEwgA_vaTA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bump transformers down for gradio/deployment inference if needed\n","!pip install transformers==4.37.2"],"metadata":{"id":"IwVigf4VvcJf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Deployment\n","LLaVA gives us 2 ways to deploy the model - via CLI or Gradio UI. We suggest using the Gradio UI for interactivity as you can compare two models and see the finetuning effect compared to the original model."],"metadata":{"id":"UxNah9r7veuR"}},{"cell_type":"code","source":["# Download the model runner\n","!wget -L https://raw.githubusercontent.com/brevdev/notebooks/main/assets/llava-deploy.sh"],"metadata":{"id":"A6cdLMgLvgju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run inference! Use the public link provided in the output to test\n","!chmod +x llava-deploy.sh && ./llava-deploy.sh"],"metadata":{"id":"Z_zb83Xkvi3X"},"execution_count":null,"outputs":[]}]}